📌 NOTEBOOK: Logistic Regression.ipynb
│
├── FASE 0: Importazione Librerie ✔️
│   └─ [Codice] import pandas as pd, numpy as np, matplotlib.pyplot, seaborn...
│
├── FASE 1: Caricamento del dataset pulito df_EDA.csv ✔️
│   └─ [Codice] df = pd.read_csv("data/df_EDA.csv")
│
├── FASE 2: Encoding delle variabili categoriche (Ordinal Encoding) 🟡
│   └─ Motivazione: usare un encoding compatibile con la regressione logistica.
│
├── FASE 3: Separazione X (features) e y (target) 🟡
│   └─ [Codice] X = df.drop('loan_status', axis=1); y = df['loan_status']
│
├── FASE 4: Train/Test Split 🟡
│   └─ [Codice] train_test_split con stratify=y
│
├── FASE 5: Standardizzazione 🟡
│   └─ Solo per Regressione Logistica. Usa StandardScaler in una pipeline.
│
├── FASE 6: Costruzione della Pipeline 🟡
│   └─ [Codice] Pipeline con StandardScaler + LogisticRegression
│
├── FASE 7: Addestramento (fit) del modello 🟡
│
├── FASE 8: Valutazione — Predizione + Matrice Confusione 🟡
│   └─ Accuracy, Recall, Precision, Confusion Matrix, ROC Curve
│
├── FASE 9: Inferenza Statistica con statsmodels 🟡
│   └─ [Codice] sm.Logit per ottenere p-value e coefficienti
│
├── FASE 10: Conclusione e interpretazione 🟡
│   └─ Quali feature influenzano? L'inferenza è significativa? Il modello è utile?
│
└── 📊 Grafici: Confusion Matrix, ROC Curve, Coefficienti (barplot con interpretazione) 🟡








📌 NOTEBOOK: Decision Tree.ipynb
│
├── FASE 0: Librerie
├── FASE 1: Caricamento df
├── FASE 2: Ordinal Encoding (come per logistica)
├── FASE 3: X e y
├── FASE 4: Train/Test Split
├── FASE 5: Costruzione modello DecisionTreeClassifier
│   ├── 📌 Pre-Pruning (max_depth, min_samples_split, ecc.)
│   └── 📌 Cross-Validation
├── FASE 6: Fit + Predict
├── FASE 7: Valutazione con accuracy, confusion matrix, ROC
├── FASE 8: Feature Importance (📈 Gini Importance)
├── FASE 9: Post-Pruning (GridSearchCV o Cost-Complexity Pruning)
├── FASE 10: Conclusione
└── 📊 Grafici: Albero, Matrice, Feature Importance, ROC








📌 NOTEBOOK: Random Forest.ipynb
├── Stesse fasi iniziali
├── Costruzione modello con RandomForestClassifier
├── Fit + Predict
├── Valutazione
├── Feature Importance (più robusta)
├── Confronto con Decision Tree
└── 📊 ROC, matrice confusione, feature importance







📌 NOTEBOOK: Neural Network.ipynb
├── Librerie (tensorflow.keras)
├── Dataset
├── Encoding
├── Standardizzazione
├── Separazione X/y
├── Costruzione modello sequenziale:
│   ├── Dense
│   ├── Activation Function (ReLU, Sigmoid)
│   ├── Loss Function (categorical crossentropy o binary crossentropy)
│   ├── Ottimizzatore (Adam con learning_rate)
├── Compilazione e Fit (epochs, batch_size)
├── Valutazione Accuracy
├── Confusion Matrix
├── Grafico della Loss e Accuracy durante gli Epoch
├── Funzione di attivazione e costo — spiegazione
├── Backpropagation (concetto)
└── Conclusione